{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs, Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316936/316936 [00:34<00:00, 9273.69it/s]\n"
     ]
    }
   ],
   "source": [
    "store = pd.HDFStore('/home/mila/v/vincent.quirion/gflownetdata/mols/data/docked_mols.h5', 'r')\n",
    "df = store.select('df')\n",
    "\n",
    "mols = []\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    mols.append(df.iloc[i].name)\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "import networkx as nx\n",
    "from gflownet.envs.frag_mol_env import FragMolBuildingEnvContext\n",
    "\n",
    "ctx = FragMolBuildingEnvContext()\n",
    "fragmols = list(enumerate(ctx.frags_mol))\n",
    "# Largest fragment first\n",
    "fragmols = sorted(fragmols, key=lambda x: -x[1].GetNumAtoms())\n",
    "\n",
    "def recursive_decompose(m, all_matches, a2f, frags, bonds, max_depth=9, numiters=None):\n",
    "    if numiters is None:\n",
    "        numiters = [0]\n",
    "    numiters[0] += 1\n",
    "    if numiters[0] > 1_000:\n",
    "        raise ValueError('too many iterations')\n",
    "    if max_depth == 0 or len(a2f) == m.GetNumAtoms():\n",
    "        # try to make a mol, does it work?\n",
    "        # Did we match all the atoms?\n",
    "        if len(a2f) < m.GetNumAtoms():\n",
    "            return None\n",
    "        # graph is a tree, e = n - 1\n",
    "        if len(bonds) != len(frags) - 1:\n",
    "            return None\n",
    "        g = nx.Graph()\n",
    "        g.add_nodes_from(range(len(frags)))\n",
    "        g.add_edges_from([(i[0], i[1]) for i in bonds])\n",
    "        assert nx.is_connected(g), 'Somehow we got here but fragments dont connect?'\n",
    "        for fi, f in enumerate(frags):\n",
    "            g.nodes[fi]['v'] = f\n",
    "        for a, b, stemidx_a, stemidx_b, _, _ in bonds:\n",
    "            g.edges[(a, b)][f'{a}_attach'] = stemidx_a\n",
    "            g.edges[(a, b)][f'{b}_attach'] = stemidx_b\n",
    "        m2 = ctx.graph_to_mol(g)\n",
    "        if m2.HasSubstructMatch(m) and m.HasSubstructMatch(m2):\n",
    "            return g\n",
    "        return None\n",
    "    for fragidx, frag in fragmols:\n",
    "        # Some fragments have symmetric versions, so we need all matches up to isomorphism!\n",
    "        matches = all_matches[fragidx]\n",
    "        for match in matches:\n",
    "            if any(i in a2f for i in match):\n",
    "                continue\n",
    "            # Verify that atoms actually have the same charge\n",
    "            if any(frag.GetAtomWithIdx(ai).GetFormalCharge() != m.GetAtomWithIdx(bi).GetFormalCharge()\n",
    "                   for ai, bi in enumerate(match)):\n",
    "                continue\n",
    "            new_frag_idx = len(frags)\n",
    "            new_frags = frags + [fragidx]\n",
    "            new_a2f = {**a2f, **{i: (fi, new_frag_idx) for fi, i in enumerate(match)}}\n",
    "            possible_bonds = []\n",
    "            is_valid_match = True\n",
    "            # Is every atom that has a bond outside of this fragment also a stem atom?\n",
    "            for fi, i in enumerate(match):\n",
    "                for j in m.GetAtomWithIdx(i).GetNeighbors():\n",
    "                    j = j.GetIdx()\n",
    "                    if j in match:\n",
    "                        continue\n",
    "                    # There should only be single bonds between fragments\n",
    "                    if m.GetBondBetweenAtoms(i, j).GetBondType() != Chem.BondType.SINGLE:\n",
    "                        is_valid_match = False\n",
    "                        break\n",
    "                    # At this point, we know (i, j) is a single bond that goes outside the fragment\n",
    "                    # so we check if the fragment we chose has that atom as a stem atom\n",
    "                    if fi not in ctx.frags_stems[fragidx]:\n",
    "                        is_valid_match = False\n",
    "                        break\n",
    "                if not is_valid_match:\n",
    "                    break\n",
    "            if not is_valid_match:\n",
    "                continue\n",
    "            for this_frag_stemidx, i in enumerate([match[s] for s in ctx.frags_stems[fragidx]]):\n",
    "                for j in m.GetAtomWithIdx(i).GetNeighbors():\n",
    "                    j = j.GetIdx()\n",
    "                    if j in match:\n",
    "                        continue\n",
    "                    if m.GetBondBetweenAtoms(i, j).GetBondType() != Chem.BondType.SINGLE:\n",
    "                        continue\n",
    "                    # Make sure the neighbor is part of an already identified fragment\n",
    "                    if j in a2f and a2f[j] != new_frag_idx:\n",
    "                        other_frag_atomidx, other_frag_idx = a2f[j]\n",
    "                        try:\n",
    "                            # Make sure that fragment has that atom as a stem atom\n",
    "                            other_frag_stemidx = ctx.frags_stems[frags[other_frag_idx]].index(other_frag_atomidx)\n",
    "                        except ValueError as e:\n",
    "                            continue\n",
    "                        # Make sure that that fragment's stem atom isn't already used\n",
    "                        for b in bonds + possible_bonds:\n",
    "                            if b[0] == other_frag_idx and b[2] == other_frag_stemidx:\n",
    "                                break\n",
    "                            if b[1] == other_frag_idx and b[3] == other_frag_stemidx:\n",
    "                                break\n",
    "                            if b[0] == new_frag_idx and b[2] == this_frag_stemidx:\n",
    "                                break\n",
    "                            if b[1] == new_frag_idx and b[3] == this_frag_stemidx:\n",
    "                                break\n",
    "                        else:\n",
    "                            possible_bonds.append((other_frag_idx, new_frag_idx, other_frag_stemidx, this_frag_stemidx, i, j))\n",
    "            new_bonds = bonds + possible_bonds\n",
    "            dec = recursive_decompose(m, all_matches, new_a2f, new_frags, new_bonds, max_depth-1, numiters)\n",
    "            if dec:\n",
    "                return dec\n",
    "def f(smi):\n",
    "    m = Chem.MolFromSmiles(smi)\n",
    "    all_matches = {}\n",
    "    for fragidx, frag in fragmols:\n",
    "        all_matches[fragidx] = m.GetSubstructMatches(frag, uniquify=False)\n",
    "    try:\n",
    "        g = recursive_decompose(m, all_matches, {}, [], [], 9)\n",
    "    except:\n",
    "        g = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_files/training_set_graphs.pkl', 'rb') as file:\n",
    "    graphs = pickle.load(file)\n",
    "valid_idxs = [i for i, g in enumerate(graphs) if g is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 246983/246983 [02:35<00:00, 1591.94it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mila/v/vincent.quirion/CURRENT_molecules/clusters.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcn-f004.server.mila.quebec/home/mila/v/vincent.quirion/CURRENT_molecules/clusters.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     DataStructs\u001b[39m.\u001b[39mConvertToNumpyArray(fp, arr)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcn-f004.server.mila.quebec/home/mila/v/vincent.quirion/CURRENT_molecules/clusters.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     fp_list\u001b[39m.\u001b[39mappend(arr)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcn-f004.server.mila.quebec/home/mila/v/vincent.quirion/CURRENT_molecules/clusters.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(fp_list)\n",
      "File \u001b[0;32m~/.conda/envs/env/lib/python3.9/site-packages/pandas/core/frame.py:781\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[39mif\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m         columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 781\u001b[0m     arrays, columns, index \u001b[39m=\u001b[39m nested_data_to_arrays(\n\u001b[1;32m    782\u001b[0m         \u001b[39m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    783\u001b[0m         \u001b[39m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    784\u001b[0m         data,\n\u001b[1;32m    785\u001b[0m         columns,\n\u001b[1;32m    786\u001b[0m         index,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    787\u001b[0m         dtype,\n\u001b[1;32m    788\u001b[0m     )\n\u001b[1;32m    789\u001b[0m     mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    790\u001b[0m         arrays,\n\u001b[1;32m    791\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[1;32m    795\u001b[0m     )\n\u001b[1;32m    796\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/env/lib/python3.9/site-packages/pandas/core/internals/construction.py:498\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mif\u001b[39;00m is_named_tuple(data[\u001b[39m0\u001b[39m]) \u001b[39mand\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     columns \u001b[39m=\u001b[39m ensure_index(data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_fields)\n\u001b[0;32m--> 498\u001b[0m arrays, columns \u001b[39m=\u001b[39m to_arrays(data, columns, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    499\u001b[0m columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    501\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/env/lib/python3.9/site-packages/pandas/core/internals/construction.py:840\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    837\u001b[0m     data \u001b[39m=\u001b[39m [\u001b[39mtuple\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data]\n\u001b[1;32m    838\u001b[0m     arr \u001b[39m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 840\u001b[0m content, columns \u001b[39m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[1;32m    841\u001b[0m \u001b[39mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/.conda/envs/env/lib/python3.9/site-packages/pandas/core/internals/construction.py:940\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    937\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(contents) \u001b[39mand\u001b[39;00m contents[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mobject_:\n\u001b[0;32m--> 940\u001b[0m     contents \u001b[39m=\u001b[39m convert_object_array(contents, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    942\u001b[0m \u001b[39mreturn\u001b[39;00m contents, columns\n",
      "File \u001b[0;32m~/.conda/envs/env/lib/python3.9/site-packages/pandas/core/internals/construction.py:1067\u001b[0m, in \u001b[0;36mconvert_object_array\u001b[0;34m(content, dtype, dtype_backend, coerce_float)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             arr \u001b[39m=\u001b[39m maybe_cast_to_datetime(arr, dtype)\n\u001b[1;32m   1065\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\n\u001b[0;32m-> 1067\u001b[0m arrays \u001b[39m=\u001b[39m [convert(arr) \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m content]\n\u001b[1;32m   1069\u001b[0m \u001b[39mreturn\u001b[39;00m arrays\n",
      "File \u001b[0;32m~/.conda/envs/env/lib/python3.9/site-packages/pandas/core/internals/construction.py:1067\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             arr \u001b[39m=\u001b[39m maybe_cast_to_datetime(arr, dtype)\n\u001b[1;32m   1065\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\n\u001b[0;32m-> 1067\u001b[0m arrays \u001b[39m=\u001b[39m [convert(arr) \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m content]\n\u001b[1;32m   1069\u001b[0m \u001b[39mreturn\u001b[39;00m arrays\n",
      "File \u001b[0;32m~/.conda/envs/env/lib/python3.9/site-packages/pandas/core/internals/construction.py:1025\u001b[0m, in \u001b[0;36mconvert_object_array.<locals>.convert\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(arr):\n\u001b[1;32m   1024\u001b[0m     \u001b[39mif\u001b[39;00m dtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mO\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1025\u001b[0m         arr \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmaybe_convert_objects(\n\u001b[1;32m   1026\u001b[0m             arr,\n\u001b[1;32m   1027\u001b[0m             try_float\u001b[39m=\u001b[39;49mcoerce_float,\n\u001b[1;32m   1028\u001b[0m             convert_to_nullable_dtype\u001b[39m=\u001b[39;49mdtype_backend \u001b[39m!=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mnumpy\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1029\u001b[0m         )\n\u001b[1;32m   1030\u001b[0m         \u001b[39m# Notes on cases that get here 2023-02-15\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m         \u001b[39m# 1) we DO get here when arr is all Timestamps and dtype=None\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m         \u001b[39m# 2) disabling this doesn't break the world, so this must be\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m         \u001b[39m#    getting caught at a higher level\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m         \u001b[39m# 3) passing convert_datetime to maybe_convert_objects get this right\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m         \u001b[39m# 4) convert_timedelta?\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m         \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fp_list = []\n",
    "for i in tqdm(valid_idxs):\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(mols[i]), 2, nBits=1024)\n",
    "    arr = np.zeros((1,), int)\n",
    "    DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "    fp_list.append(arr)\n",
    "\n",
    "df = pd.DataFrame(fp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_cluster(train_list, num_clusters, sample_size=None):\n",
    "    arr = np.array(train_list, dtype=np.float16)\n",
    "\n",
    "    km = MiniBatchKMeans(n_clusters=num_clusters, random_state=0, batch_size=600)\n",
    "    km.fit(arr)\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/v/vincent.quirion/.conda/envs/env/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "km = kmeans_cluster(fp_list, num_clusters=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2081, 16450,  6111, 17022,  6717,  4397, 10927, 12753,   319,\n",
       "       17279,  7227,  4008,  5367, 11471, 12715, 13730, 18289,  5264,\n",
       "       21262,  5847,  9245, 13792,  5002,  8329, 11379])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_files/25_kmeans_model.pkl', 'wb') as f:\n",
    "    pickle.dump(km, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
